
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Classification &mdash; Orange Documentation v2.7.8</title>
    
    <link rel="stylesheet" href="../../_static/orange.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '2.7.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="top" title="Orange Documentation v2.7.8" href="../../index.html" />
    <script type="text/javascript" src="../../_static/copybutton.js"></script>

  </head>
  <body>
	<div id="container">
    <div class="border1"></div>
    <div class="border2"></div>
    <div class="borderv">
        <div id="header">
			<div id="orangeimg"><h1><a href="http://orange.biolab.si"><img src="../../_static/orange-logo-w.png" alt="Orange" /></a></h1></div>

            <div id="cse-search-box" style="height: 22px;"></div>
            <div id="underimg"></div>
        </div>
    </div>
    <div class="border2"></div>
    <div class="border1"></div>

    <div id="main">
        <div class="border1"></div>
        <div class="border2"></div>
        <div class="borderv">
            <div id="maininner">
            <p style="font-size: 32px;">
                This is documentation for Orange 2.7. For the latest documentation, 
                <a href="http://orange.biolab.si/docs">
                see Orange 3</a>.
            </p>
 

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
    <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
           <p><a class="uplink" href="../../index.html">Orange Documentation v2.7.8</a></p>
           <ul>
<li><a class="reference internal" href="#">Classification</a><ul>
<li><a class="reference internal" href="#learners-and-classifiers">Learners and Classifiers</a></li>
<li><a class="reference internal" href="#probabilistic-classification">Probabilistic Classification</a></li>
<li><a class="reference internal" href="#cross-validation">Cross-Validation</a></li>
<li><a class="reference internal" href="#handful-of-classifiers">Handful of Classifiers</a></li>
<li><a class="reference internal" href="#reporting-on-classification-models">Reporting on Classification Models</a></li>
</ul>
</li>
</ul>

        </div>
    </div>
    
  <div class="section" id="classification">
<h1>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h1>
<span class="target" id="index-0"></span><p id="index-1">Much of Orange is devoted to machine learning methods for classification, or supervised data mining. These methods rely on
the data with class-labeled instances, like that of senate voting. Here is a code that loads this data set, displays the first data instance and shows its predicted class (<tt class="docutils literal"><span class="pre">republican</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;voting&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">[&#39;n&#39;, &#39;y&#39;, &#39;n&#39;, &#39;y&#39;, &#39;y&#39;, &#39;y&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;y&#39;, &#39;?&#39;, &#39;y&#39;, &#39;y&#39;, &#39;y&#39;, &#39;n&#39;, &#39;y&#39;, &#39;republican&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_class</span><span class="p">()</span>
<span class="go">&lt;orange.Value &#39;party&#39;=&#39;republican&#39;&gt;</span>
</pre></div>
</div>
<div class="section" id="learners-and-classifiers">
<h2>Learners and Classifiers<a class="headerlink" href="#learners-and-classifiers" title="Permalink to this headline">¶</a></h2>
<span class="target" id="index-2"></span><span class="target" id="index-3"></span><p id="index-4">Classification uses two types of objects: learners and classifiers. Learners consider class-labeled data and return a classifier. Given a data instance (a vector of feature values), classifiers return a predicted class:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">Orange</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;voting&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learner</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">bayes</span><span class="o">.</span><span class="n">NaiveLearner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">learner</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="go">&lt;orange.Value &#39;party&#39;=&#39;republican&#39;&gt;</span>
</pre></div>
</div>
<p>Above, we read the data, constructed a <a class="reference external" href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive Bayesian learner</a>, gave it the data set to construct a classifier, and used it to predict the class of the first data item. We also use these concepts in the following code that predicts the classes of the first five instances in the data set:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">classifier</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">bayes</span><span class="o">.</span><span class="n">NaiveLearner</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%10s</span><span class="s">; originally </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">classifier</span><span class="p">(</span><span class="n">d</span><span class="p">),</span> <span class="n">d</span><span class="o">.</span><span class="n">getclass</span><span class="p">())</span>
</pre></div>
</div>
<p>The script outputs:</p>
<div class="highlight-python"><pre>republican; originally republican
republican; originally republican
republican; originally democrat
  democrat; originally democrat
  democrat; originally democrat</pre>
</div>
<p>Naive Bayesian classifier has made a mistake in the third instance, but otherwise predicted correctly. No wonder, since this was also the data it trained from.</p>
</div>
<div class="section" id="probabilistic-classification">
<h2>Probabilistic Classification<a class="headerlink" href="#probabilistic-classification" title="Permalink to this headline">¶</a></h2>
<p>To find out what is the probability that the classifier assigns
to, say, democrat class, we need to call the classifier with
additional parameter that specifies the output type. If this is <tt class="docutils literal"><span class="pre">Orange.classification.Classifier.GetProbabilities</span></tt>, the classifier will output class probabilities:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">classifier</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">bayes</span><span class="o">.</span><span class="n">NaiveLearner</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">print</span> <span class="s">&quot;Probabilities for </span><span class="si">%s</span><span class="s">:&quot;</span> <span class="o">%</span> <span class="n">data</span><span class="o">.</span><span class="n">domain</span><span class="o">.</span><span class="n">class_var</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">Classifier</span><span class="o">.</span><span class="n">GetProbabilities</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%5.3f</span><span class="s">; originally </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">target</span><span class="p">],</span> <span class="n">d</span><span class="o">.</span><span class="n">getclass</span><span class="p">())</span>
</pre></div>
</div>
<p>The output of the script also shows how badly the naive Bayesian classifier missed the class for the thrid data item:</p>
<div class="highlight-python"><pre>Probabilities for democrat:
0.000; originally republican
0.000; originally republican
0.005; originally democrat
0.998; originally democrat
0.957; originally democrat</pre>
</div>
</div>
<div class="section" id="cross-validation">
<h2>Cross-Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h2>
<p>Validating the accuracy of classifiers on the training data, as we did above, serves demonstration purposes only. Any performance measure that assess accuracy should be estimated on the independent test set. Such is also a procedure called <a class="reference external" href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>, which averages performance estimates across several runs, each time considering a different training and test subsets as sampled from the original data set:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;voting&quot;</span><span class="p">)</span>
<span class="n">bayes</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">bayes</span><span class="o">.</span><span class="n">NaiveLearner</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">([</span><span class="n">bayes</span><span class="p">],</span> <span class="n">data</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Accuracy: </span><span class="si">%.2f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">scoring</span><span class="o">.</span><span class="n">CA</span><span class="p">(</span><span class="n">res</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span> <span class="s">&quot;AUC:      </span><span class="si">%.2f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">scoring</span><span class="o">.</span><span class="n">AUC</span><span class="p">(</span><span class="n">res</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<span class="target" id="index-6"></span><span class="target" id="index-7"></span><p id="index-8">Cross-validation is expecting a list of learners. The performance estimators also return a list of scores, one for every learner. There was just one learner in the script above, hence the list of size one was used. The script estimates classification accuracy and area under ROC curve. The later score is very high, indicating a very good performance of naive Bayesian learner on senate voting data set:</p>
<div class="highlight-python"><pre>Accuracy: 0.90
AUC:      0.97</pre>
</div>
</div>
<div class="section" id="handful-of-classifiers">
<h2>Handful of Classifiers<a class="headerlink" href="#handful-of-classifiers" title="Permalink to this headline">¶</a></h2>
<p>Orange includes wide range of classification algorithms, including:</p>
<ul class="simple">
<li>logistic regression (<tt class="docutils literal"><span class="pre">Orange.classification.logreg</span></tt>)</li>
<li>k-nearest neighbors (<tt class="docutils literal"><span class="pre">Orange.classification.knn</span></tt>)</li>
<li>support vector machines (<tt class="docutils literal"><span class="pre">Orange.classification.svm</span></tt>)</li>
<li>classification trees (<tt class="docutils literal"><span class="pre">Orange.classification.tree</span></tt>)</li>
<li>classification rules (<tt class="docutils literal"><span class="pre">Orange.classification.rules</span></tt>)</li>
</ul>
<p>Some of these are included in the code that estimates the probability of a target class on a testing data. This time, training and test data sets are disjoint:</p>
<span class="target" id="index-9"></span><span class="target" id="index-10"></span><div class="highlight-python" id="index-11"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">Orange</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;voting&quot;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">([</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">d</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">test</span><span class="p">])</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">regression</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">TreeLearner</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">same_majority_pruning</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">m_pruning</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;tree&quot;</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">knn</span><span class="o">.</span><span class="n">kNNLearner</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;k-NN&quot;</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">logreg</span><span class="o">.</span><span class="n">LogRegLearner</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;lr&quot;</span>

<span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span><span class="n">tree</span><span class="p">,</span> <span class="n">knn</span><span class="p">,</span> <span class="n">lr</span><span class="p">]</span>

<span class="n">target</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">print</span> <span class="s">&quot;Probabilities for </span><span class="si">%s</span><span class="s">:&quot;</span> <span class="o">%</span> <span class="n">data</span><span class="o">.</span><span class="n">domain</span><span class="o">.</span><span class="n">class_var</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
<span class="k">print</span> <span class="s">&quot;original class &quot;</span><span class="p">,</span>
<span class="k">print</span> <span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%-9s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">l</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">)</span>

<span class="n">return_type</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">Classifier</span><span class="o">.</span><span class="n">GetProbabilities</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">test</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">&quot;</span><span class="si">%-15s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">getclass</span><span class="p">()),</span>
    <span class="k">print</span> <span class="s">&quot;     &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%5.3f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">c</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">return_type</span><span class="p">)[</span><span class="n">target</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">)</span>
</pre></div>
</div>
<p>For these five data items, there are no major differences between predictions of observed classification algorithms:</p>
<div class="highlight-python"><pre>Probabilities for republican:
original class  tree      k-NN      lr
republican      0.949     1.000     1.000
republican      0.972     1.000     1.000
democrat        0.011     0.078     0.000
democrat        0.015     0.001     0.000
democrat        0.015     0.032     0.000</pre>
</div>
<p>The following code cross-validates several learners. Notice the difference between this and the code above. Cross-validation requires learners, while in the script above, learners were immediately given the data and the calls returned classifiers.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">Orange</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;voting&quot;</span><span class="p">)</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">TreeLearner</span><span class="p">(</span><span class="n">sameMajorityPruning</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mForPruning</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;tree&quot;</span>
<span class="n">nbc</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">bayes</span><span class="o">.</span><span class="n">NaiveLearner</span><span class="p">()</span>
<span class="n">nbc</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;nbc&quot;</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">logreg</span><span class="o">.</span><span class="n">LogRegLearner</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">&quot;lr&quot;</span>

<span class="n">learners</span> <span class="o">=</span> <span class="p">[</span><span class="n">nbc</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">lr</span><span class="p">]</span>
<span class="k">print</span> <span class="s">&quot; &quot;</span><span class="o">*</span><span class="mi">9</span> <span class="o">+</span> <span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%-4s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">learner</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">learner</span> <span class="ow">in</span> <span class="n">learners</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">cross_validation</span><span class="p">(</span><span class="n">learners</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Accuracy </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%.2f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">scoring</span><span class="o">.</span><span class="n">CA</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
<span class="k">print</span> <span class="s">&quot;AUC      </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%.2f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">Orange</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">scoring</span><span class="o">.</span><span class="n">AUC</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
</pre></div>
</div>
<p>Logistic regression wins in area under ROC curve:</p>
<div class="highlight-python"><pre>         nbc  tree lr
Accuracy 0.90 0.95 0.94
AUC      0.97 0.94 0.99</pre>
</div>
</div>
<div class="section" id="reporting-on-classification-models">
<h2>Reporting on Classification Models<a class="headerlink" href="#reporting-on-classification-models" title="Permalink to this headline">¶</a></h2>
<p>Classification models are objects, exposing every component of its structure. For instance, one can traverse classification tree in code and observe the associated data instances, probabilities and conditions. It is often, however, sufficient, to provide textual output of the model. For logistic regression and trees, this is illustrated in the script below:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">Orange</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s">&quot;titanic&quot;</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">logreg</span><span class="o">.</span><span class="n">LogRegLearner</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">logreg</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">TreeLearner</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span> <span class="n">tree</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span>
</pre></div>
</div>
<p>The logistic regression part of the output is:</p>
<div class="highlight-python"><pre>class attribute = survived
class values = &lt;no, yes&gt;

      Feature       beta  st. error     wald Z          P OR=exp(beta)

    Intercept      -1.23       0.08     -15.15      -0.00
 status=first       0.86       0.16       5.39       0.00       2.36
status=second      -0.16       0.18      -0.91       0.36       0.85
 status=third      -0.92       0.15      -6.12       0.00       0.40
    age=child       1.06       0.25       4.30       0.00       2.89
   sex=female       2.42       0.14      17.04       0.00      11.25</pre>
</div>
<p>Trees can also be rendered in <a class="reference external" href="http://en.wikipedia.org/wiki/DOT_language">dot</a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">tree</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">file_name</span><span class="o">=</span><span class="s">&quot;0.dot&quot;</span><span class="p">,</span> <span class="n">node_shape</span><span class="o">=</span><span class="s">&quot;ellipse&quot;</span><span class="p">,</span> <span class="n">leaf_shape</span><span class="o">=</span><span class="s">&quot;box&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Following figure shows an example of such rendering.</p>
<img alt="A graphical presentation of a classification tree" src="../../_images/tree.png" />
</div>
</div>



          </div>
        </div>
      </div> 
      <div class="clearer"></div>
    </div>  
	
    <div class="footer">
    </div>
	            </div>
        </div>
        <div class="border1"></div>
        <div class="border2"></div>
    </div>
</div>

  </body>
</html>